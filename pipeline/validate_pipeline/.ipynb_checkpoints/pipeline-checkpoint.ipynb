{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary packages\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usable days from winter and summer sampling\n",
    "IS_SUMMER = False\n",
    "SEASON = 'winter'\n",
    "\n",
    "DAYS_TO_KEEP = [f'2019-01-{day:02d}' for day in range(7,32)] +  \\\n",
    "                [f'2019-02-{day:02d}' for day in range(1,3)] +  \\\n",
    "                [f'2019-02-{day:02d}' for day in range(13, 29)] + \\\n",
    "                [f'2019-03-{day:02d}' for day in range(1, 3)]\n",
    "if IS_SUMMER:\n",
    "    SEASON = 'summer'\n",
    "    DAYS_TO_KEEP = [f'2019-07-{day:02d}' for day in range(1, 32)]  + \\\n",
    "                   [f'2019-08-{day:02d}' for day in range(1, 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APC/AVL and OCRA files to merge\n",
    "FILEPATH = 'C:/Users/mstark/Desktop/DATA591/'\n",
    "ORCA_FILE = FILEPATH + 'orcadata_' + SEASON + '.csv'\n",
    "AVL_FILE = FILEPATH + 'apcdata_' + SEASON + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ORCA CLEAN\n",
    "\n",
    "# read in raw ORCA transactions\n",
    "ORCA_DATA = pd.read_csv(ORCA_FILE, low_memory=False)\n",
    "print('Rows, Features:', ORCA_DATA.shape)\n",
    "O_0 = ORCA_DATA.shape[0]\n",
    "\n",
    "# if any duplicate in OCRA data, drop\n",
    "ORCA_DUPS = ORCA_DATA[ORCA_DATA.duplicated()].shape[0]\n",
    "if ORCA_DUPS > 0:\n",
    "    ORCA_DATA.drop_duplicates()\n",
    "print('Duplicate records:', ORCA_DUPS)\n",
    "print('Rows, Features:', ORCA_DATA.shape)\n",
    "print('Deduplicated Passenger Count:', ORCA_DATA['passenger_count'].sum())\n",
    "\n",
    "# reduce columns\n",
    "ORCA_COLS_TO_KEEP = ['institution_name', 'business_date', 'txn_passenger_type_descr',\n",
    "                     'passenger_count', 'service_agency_id', 'mode_id', 'route_number',\n",
    "                     'stop_id', 'trip_id']\n",
    "ORCA_DATA = ORCA_DATA[ORCA_COLS_TO_KEEP]\n",
    "O_1 = ORCA_DATA.shape[0]\n",
    "print('Rows, Features:', ORCA_DATA.shape)\n",
    "\n",
    "# keep days, service_agency_id = 4, mode_id in [128, 250], and route_numbers [<600, between 671 and 676]\n",
    "ORCA_DATA['route_number'] = pd.to_numeric(ORCA_DATA['route_number'], errors='coerce')\n",
    "ORCA_DATA = ORCA_DATA[(ORCA_DATA['business_date'].isin(DAYS_TO_KEEP)) \n",
    "                      & (ORCA_DATA['service_agency_id'] == 4)\n",
    "                      & (ORCA_DATA['mode_id'].isin([128, 250]))\n",
    "                      & ((ORCA_DATA['route_number'] < 600) \n",
    "                         | ((ORCA_DATA['route_number'] >= 671) & (ORCA_DATA['route_number'] <= 676)))]\n",
    "O_2 = ORCA_DATA.shape[0]\n",
    "print('Rows, Features:', ORCA_DATA.shape)\n",
    "print(\"Filtered Passenger Count:\", ORCA_DATA['passenger_count'].sum())\n",
    "\n",
    "# add features\n",
    "ORCA_DATA['orca_total'] = ORCA_DATA['passenger_count']\n",
    "ORCA_DATA['orca_adult'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Adult', 0)\n",
    "ORCA_DATA['orca_disabled'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Disabled', 0)\n",
    "ORCA_DATA['orca_senior'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Senior', 0)\n",
    "ORCA_DATA['orca_youth'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Youth', 0)\n",
    "ORCA_DATA['orca_lowincome'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Low Income', 0)\n",
    "ORCA_DATA['orca_uw'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['institution_name'] == 'University of Washington', 0)\n",
    "\n",
    "# cleaned ORCA results\n",
    "print('Preserved', O_2, 'rows of', O_0, 'from OCRA transtions.', O_2*1.0/O_0*100, '%')\n",
    "\n",
    "ORCA_SUMS = ORCA_DATA[['passenger_count', 'orca_total', 'orca_adult', 'orca_disabled', 'orca_senior', 'orca_youth', 'orca_lowincome', 'orca_uw']].sum()\n",
    "OS_VALUES = list(ORCA_SUMS)\n",
    "VALIDATE = sum(OS_VALUES[2:7])\n",
    "\n",
    "IS_VALID = False\n",
    "if VALIDATE == OS_VALUES[0]:\n",
    "    IS_VALID = True\n",
    "    \n",
    "# print(int(list(ORCA_SUMS)[0])\n",
    "print(\"Deduplicated Passenger Count:\")\n",
    "print(ORCA_SUMS)\n",
    "print(\"Counts are valid:\", IS_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtered Passenger Count:\", ORCA_DATA['passenger_count'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features\n",
    "ORCA_DATA['orca_total'] = ORCA_DATA['passenger_count']\n",
    "ORCA_DATA['orca_adult'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Adult', 0)\n",
    "ORCA_DATA['orca_disabled'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Disabled', 0)\n",
    "ORCA_DATA['orca_senior'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Senior', 0)\n",
    "ORCA_DATA['orca_youth'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Youth', 0)\n",
    "ORCA_DATA['orca_lowincome'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['txn_passenger_type_descr'] == 'Low Income', 0)\n",
    "ORCA_DATA['orca_uw'] = ORCA_DATA['passenger_count'].where(ORCA_DATA['institution_name'] == 'University of Washington', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned ORCA results\n",
    "print('Preserved', O_2, 'rows of', O_0, 'from OCRA transtions.', O_2*1.0/O_0*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORCA_SUMS = ORCA_DATA[['passenger_count', 'orca_total', 'orca_adult', 'orca_disabled', 'orca_senior', 'orca_youth', 'orca_lowincome', 'orca_uw']].sum()\n",
    "print(\"Deduplicated Passenger Count:\", ORCA_SUMS, sum(list(ORCA_SUMS)[2:7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Orca data is missing:\")\n",
    "print(ORCA_DATA[['trip_id', 'stop_id', 'route_number']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate ORCA over day, trip, stop, route\n",
    "ORCA_AGG_GROUPBY = ['business_date', 'trip_id', 'stop_id', 'route_number']\n",
    "ORCA_AGG_SUMOVER = ['orca_total', 'orca_adult', 'orca_disabled', 'orca_senior', 'orca_youth', 'orca_lowincome', 'orca_uw']\n",
    "ORCA_AGG = ORCA_DATA[ORCA_AGG_GROUPBY + ORCA_AGG_SUMOVER].groupby(ORCA_AGG_GROUPBY).sum().reset_index()\n",
    "print('Rows:', ORCA_AGG.shape[0], 'Passenger Count:', ORCA_AGG['orca_total'].sum())\n",
    "\n",
    "# Rows: 45      Passenger Count: 9720977 Group By: business_ate\n",
    "# Rows: 442727  Passenger Count: 9674582 Group By: business_date, trip_id\n",
    "# Rows: 3856386 Passenger Count: 9667016 Group By: business_date, trip_id, stop_id\n",
    "# Rows: 3856386 Passenger Count: 9667016 Group By: business_date, trip_id, stop_id, route_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in AVL/APC data\n",
    "avl_data = pd.read_csv(avl_file, header = None)\n",
    "avl_header = ['daycode', 'trip_id', 'pattern_id', 'pattern_quality', 'blk', 'rte', 'dir', '???', 'opd_date',\n",
    "              'pattern_quality_1', 'vehicle_id', 'stop_id', 'stop_seq', 'STOP_NAME', 'sch_stop_sec', 'act_stop_arr',\n",
    "              'sch_stop_tm', 'act_stop_tm', 'dwell_sec', 'doors_open', 'door_open_sec', 'apc_veh', 'ons', 'offs',\n",
    "              'load', 'ST/S/STE', '??', '?', 'stop_datetime', 'record_id', 'id']\n",
    "avl_data.columns = avl_header\n",
    "print('Rows, Features:', avl_data.shape)\n",
    "a_0 = avl_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original AVL:\", avl_data['ons'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates in raw file\n",
    "avl_dups = avl_data[avl_data.duplicated()]\n",
    "a_05 = avl_dups.shape[0]\n",
    "print('Duplicate records:', avl_dups.shape[0])\n",
    "avl_dups.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate records\n",
    "avl_data = avl_data.drop_duplicates()\n",
    "a_1 = avl_data.shape[0]\n",
    "print('Validate all rows accounted for:', a_0, a_05+a_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deduplicated AVL:\", avl_data['ons'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep days, apc vehic le, and rte [<600, between 671 and 676]\n",
    "avl_data = avl_data[(avl_data['opd_date'].isin(days_to_keep))\n",
    "                   & (avl_data['apc_veh'] == 'Y')\n",
    "                   & ((avl_data['rte'].astype(int) < 600) \n",
    "                      | ((avl_data['rte'].astype(int) >= 671) & (avl_data['rte'].astype(int) <= 676)))]\n",
    "a_2 = avl_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtered AVL:\", avl_data['ons'].sum()) #with dups 11243035.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add features\n",
    "avl_data['day_of_week'] = pd.to_datetime(avl_data['opd_date']).dt.dayofweek\n",
    "avl_data['is_rapidride'] = [1 if x > 600 else 0 for x in avl_data['rte'].astype(int)]\n",
    "avl_data['opd_txn_diff'] = (pd.to_datetime(avl_data['opd_date']).dt.date\n",
    "                             - pd.to_datetime(avl_data['stop_datetime']).dt.date)/np.timedelta64(1, 'D')\n",
    "avl_data['ons_update'] = [x if x < 150 else None for x in avl_data['ons']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned APC results\n",
    "print('Preserved', a_2, 'row of', a_0, 'from APC transtions.', a_2*1.0/a_0*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate APC over day, trip, stop, route\n",
    "avl_agg_groupby = ['opd_date', 'trip_id', 'stop_id', 'rte', 'dir', 'is_rapidride', 'day_of_week']\n",
    "avl_agg_sumover = ['ons', 'offs', 'load', 'ons_update']\n",
    "avl_agg = avl_data[avl_agg_groupby + avl_agg_sumover].groupby(avl_agg_groupby).sum().reset_index()\n",
    "print('Rows:', avl_agg.shape[0], avl_agg['ons'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = avl_data[['trip_id', 'stop_id', 'opd_date', 'rte', 'ons']].groupby(['trip_id', 'stop_id', 'opd_date', 'rte']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1[test1['ons'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge orca_agg and apc_agg\n",
    "merged_data = pd.merge(avl_data, orca_agg, \n",
    "                       left_on = ['trip_id', 'stop_id', 'opd_date', 'rte'],\n",
    "                       right_on = ['trip_id', 'stop_id', 'business_date', 'route_number'],\n",
    "                       how = 'left',\n",
    "                       suffixes = ('_apc', '_orca'))\n",
    "print('Rows, Features:', merged_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data['orca_total'].isna().sum())\n",
    "merged_data['orca_total'] = merged_data['orca_total'].fillna(0)\n",
    "merged_data['orca_total'].isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[['ons', 'orca_total']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check merged_data is unique\n",
    "distinct_col = ['opd_date', 'trip_id', 'stop_id', 'rte']\n",
    "merge_dups = merged_data[merged_data[distinct_col].duplicated()]\n",
    "print('Duplicate records:', merge_dups.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without rte\n",
    "# ons           11240430.0\n",
    "# orca_total     1286910.0\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll up ons/counts to trip_id (drop stop_id)\n",
    "trip_groupby = ['opd_date', 'day_of_week', 'trip_id', 'rte', 'is_rapidride', 'dir']\n",
    "trip_sumover = ['ons', 'offs', 'ons_update'] + list(merged_data.columns)[-7:]\n",
    "print(trip_sumover)\n",
    "trip_agg = merged_data[trip_groupby + trip_sumover].groupby(trip_groupby).sum().reset_index()\n",
    "print('Rows, Features:', trip_agg.shape)\n",
    "trip_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_agg[['ons', 'orca_total']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot apc vs ORCA\n",
    "plt.scatter(trip_agg['ons'], trip_agg['orca_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find valid trips, where ocra <= apc and apc >= 0\n",
    "trip_agg_sub0 = trip_agg[trip_agg['orca_total'] > trip_agg['ons']]\n",
    "print('Rows where OCRA > APC for trip:', trip_agg_sub0.shape[0])\n",
    "\n",
    "trip_agg_sub1 = trip_agg[(trip_agg['ons'] >= 0) & (trip_agg['orca_total'] <= trip_agg['ons'])]\n",
    "print('Rows where OCRA < APC and APC >= 0 for trip:', trip_agg_sub1.shape[0])\n",
    "\n",
    "trip_agg_sub2 = trip_agg[(trip_agg['ons_update'] >= 0) & (trip_agg['orca_total'] <= trip_agg['ons_update'])]\n",
    "print('Rows where OCRA < APC and APC >= 0 for trip (ons < 150 at every apc):', trip_agg_sub2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot valid apc vs ORCA\n",
    "plt.scatter(trip_agg_sub1['ons'], trip_agg_sub1['orca_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot valid apc vs ORCA (ons < 150 at every stop)\n",
    "plt.scatter(trip_agg_sub1['ons_update'], trip_agg_sub1['orca_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of ORCA/APC ratio for trip/day\n",
    "# trip_agg_sub1['ratio_0'] = trip_agg_sub1['orca_total']/trip_agg_sub1['ons']\n",
    "trip_agg_sub2['ratio_1'] = trip_agg_sub2['orca_total']/trip_agg_sub2['ons_update']\n",
    "print('Average ratio:', np.mean(trip_agg_sub2['ratio_1']))\n",
    "# plt.hist(trip_agg_sub2['ratio_1']) #, bins = 100)\n",
    "# plt.xlim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_agg_sub2[['ons_update', 'orca_total']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = trip_agg_sub2[['rte']].groupby(['rte']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(filepath + 'data/rte.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(test['rte'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.reset_index()\n",
    "# test\n",
    "print(test.shape, np.unique(test['rte']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets\n",
    "orca_data.to_csv(filepath + 'data/ORCA_cleaned.csv')\n",
    "print(orca_data.shape)\n",
    "orca_agg.to_csv(filepath + 'data/ORCA_aggregate.csv')\n",
    "print(orca_agg.shape)\n",
    "avl_data.to_csv(filepath + 'data/APC_cleaned.csv')\n",
    "print(avl_data.shape)\n",
    "avl_agg.to_csv(filepath + 'data/APC_aggregate.csv')\n",
    "print(avl_agg.shape)\n",
    "trip_agg_sub2.to_csv(filepath + 'data/trip_rollup.csv')\n",
    "print(trip_agg_sub2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get earliest trip time\n",
    "trip_time = avl_data[['opd_date', 'trip_id', 'stop_datetime']].groupby(['opd_date', 'trip_id']).min().reset_index()\n",
    "\n",
    "def hour_rounder(interval, t):\n",
    "    if interval == .5:\n",
    "        if t.minute > 30:\n",
    "            x = t.replace(second=0, microsecond=0, minute=30, hour=t.hour).strftime('%H:%M:%S')\n",
    "        else:\n",
    "            x = t.replace(second=0, microsecond=0, minute=0, hour=t.hour).strftime('%H:%M:%S')\n",
    "    else:\n",
    "        y = t.hour\n",
    "        if t.hour % interval != 0:\n",
    "            y = t.hour - t.hour % interval\n",
    "        x = t.replace(second=0, microsecond=0, minute=0, hour=y).strftime('%H:%M:%S')\n",
    "    return x\n",
    "\n",
    "trip_time['halfhr'] = [hour_rounder(.5, x) for x in pd.to_datetime(trip_time['stop_datetime'])]\n",
    "trip_time['1hr'] = [hour_rounder(1, x) for x in pd.to_datetime(trip_time['stop_datetime'])]\n",
    "trip_time['2hr'] = [hour_rounder(2, x) for x in pd.to_datetime(trip_time['stop_datetime'])]\n",
    "trip_time['4hr'] = [hour_rounder(4, x) for x in pd.to_datetime(trip_time['stop_datetime'])]\n",
    "trip_time['6hr'] = [hour_rounder(6, x) for x in pd.to_datetime(trip_time['stop_datetime'])]\n",
    "trip_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training sets\n",
    "time_intervals = ['halfhr', '1hr', '2hr', '4hr', '6hr']\n",
    "\n",
    "for val in time_intervals:\n",
    "    set_data = trip_agg_sub2.drop(['route_number', 'ratio_1'], axis = 1)\n",
    "    set_data = pd.merge(set_data , trip_time[['opd_date', 'trip_id', val]], \n",
    "                        left_on=['trip_id', 'opd_date'],\n",
    "                        right_on=['trip_id', 'opd_date'],\n",
    "                        how='inner',\n",
    "                        suffixes=('_data', '_time'))\n",
    "    set_data['ons'] = set_data['ons_update']\n",
    "    set_data = set_data.drop(['trip_id', 'ons_update'], axis = 1).groupby(['opd_date', 'rte', 'dir', 'day_of_week', 'is_rapidride', val]).sum().reset_index()\n",
    "    \n",
    "    cols = list(set_data.columns)\n",
    "    cols.remove('ons')\n",
    "    X = set_data[cols]\n",
    "    y = set_data['ons']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)\n",
    "    \n",
    "    print(np.sum(y))\n",
    "#     print(set_data.head(5))\n",
    "    print(val + \"(train, test, val):\", X_train.shape[0], X_test.shape[0], X_val.shape[0])\n",
    "    X_train.to_csv(filepath + '/data/' + val + '/X_train.csv')\n",
    "    X_test.to_csv(filepath + '/data/' + val + '/X_test.csv')\n",
    "    X_val.to_csv(filepath + '/data/' + val + '/X_val.csv')\n",
    "    y_train.to_csv(filepath + '/data/' + val + '/y_train.csv')\n",
    "    y_test.to_csv(filepath + '/data/' + val + '/y_test.csv')\n",
    "    y_val.to_csv(filepath + '/data/' + val + '/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare previous pipeline results with this\n",
    "compare = pd.read_csv(filepath + '49_merged_at_stop_level.tsv.gz', sep='\\t')\n",
    "compare_dups = compare[['opd_date', 'trip_id', 'stop_id', 'ons', 'orca_total']][compare[['opd_date', 'trip_id', 'stop_id']].duplicated()]\n",
    "print(compare_dups.shape)\n",
    "compare_dups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rte 49 from prior pipeline\n",
    "compare[(compare['trip_id'] == 40684352) \n",
    "        & (compare['opd_date'] == '2019-03-01') \n",
    "        & (compare['stop_id'] == 1180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rte 49 from current pipeline\n",
    "merged_data[['opd_date', 'trip_id', 'stop_id', 'ons', 'orca_total',\n",
    "             'orca_adult', 'orca_disabled', 'orca_senior',\n",
    "             'orca_youth', 'orca_lowincome',\n",
    "             'orca_uw']][(merged_data['trip_id'] == 40684352) \n",
    "                          & (merged_data['opd_date'] == '2019-03-01') \n",
    "                          & (merged_data['stop_id'] == 1180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rte 49 from raw ORCA\n",
    "orca_data[['business_date', 'trip_id', 'stop_id',\n",
    "           'passenger_count', 'txn_passenger_type_descr']][(orca_data['business_date'] == '2019-03-01')\n",
    "                                                           & (orca_data['trip_id'] == 40684352)\n",
    "                                                           & (orca_data['stop_id'] == 1180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rte 49 from raw APC\n",
    "avl_data[['opd_date', 'trip_id', 'stop_id', 'ons']][(avl_data['opd_date'] == '2019-03-01')\n",
    "         & (avl_data['trip_id'] == 40684352)\n",
    "         & (avl_data['stop_id'] == 1180)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total rte 49 APC and ORCA for 3/1/19 & 3/2/19 from prior pipeline\n",
    "compare[['ons', 'orca_total']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total rte 49 APC and ORCA for 3/1/19 & 3/2/19 from this pipeline\n",
    "trip_agg_sub2[['ons', 'orca_total']][((trip_agg_sub2['opd_date'] == '2019-03-01') \n",
    "                                      | (trip_agg_sub2['opd_date'] == '2019-03-02')) \n",
    "                                     &(trip_agg_sub2['rte'] ==49)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_agg_sub2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[['opd_date', 'trip_id', 'stop_id', 'ons', 'orca_total',\n",
    "             'orca_adult', 'orca_disabled', 'orca_senior',\n",
    "             'orca_youth', 'orca_lowincome',\n",
    "             'orca_uw']][(merged_data['opd_date'] == '2019-03-01') \n",
    "                         & (merged_data['trip_id'] == 34745817)\n",
    "                        & (merged_data['stop_id'] == 75995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avl_data[['opd_date', 'trip_id', 'stop_id', 'ons']][(avl_data['opd_date'] == '2019-03-01')\n",
    "                                                   & (avl_data['trip_id'] == 34745817)\n",
    "                                                   & (avl_data['stop_id'] == 75995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orca_data[['business_date', 'trip_id', 'stop_id',\n",
    "           'passenger_count', 'txn_passenger_type_descr']][(orca_data['business_date'] == '2019-03-01')\n",
    "                                                           & (orca_data['trip_id'] == 34745817)\n",
    "                                                           & (orca_data['stop_id'] == 75995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rte 49 from current pipeline\n",
    "merged_data[['opd_date', 'trip_id', 'stop_id', 'rte', 'ons', 'orca_total',\n",
    "             'orca_adult', 'orca_disabled', 'orca_senior',\n",
    "             'orca_youth', 'orca_lowincome',\n",
    "             'orca_uw']][(merged_data['rte'] == 49) \n",
    "                          & (merged_data['opd_date'].isin(['2019-03-01', '2019-03-02']))].to_csv('49_merged_at_stop_level_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avl_data[['opd_date', 'trip_id', 'stop_id', 'rte', 'ons', 'offs']][(avl_data['rte'] == 49) \n",
    "                                                                  & (avl_data['opd_date'].isin(['2019-03-01', '2019-03-02']))]#.to_csv('49_merged_at_stop_level_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare previous pipeline results with this\n",
    "compare = pd.read_csv(filepath + 'RS_merged_at_stop_level.tsv.gz', sep='\\t')\n",
    "compare_dups = compare[['opd_date', 'trip_id', 'stop_id', 'ons', 'orca_total']][compare[['opd_date', 'trip_id', 'stop_id']].duplicated()]\n",
    "print(compare_dups.shape)\n",
    "compare_dups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_ids = np.unique(compare['trip_id'])\n",
    "stop_ids = np.unique(compare['stop_id'])\n",
    "days = np.unique(compare['opd_date'])\n",
    "print(trip_ids, stop_ids, days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[['opd_date', 'trip_id', 'stop_id', 'rte', 'ons', 'orca_total',\n",
    "             'orca_adult', 'orca_disabled', 'orca_senior',\n",
    "             'orca_youth', 'orca_lowincome',\n",
    "             'orca_uw']][(merged_data['trip_id'].isin(ten_trips))\n",
    "                         & (merged_data['opd_date'].isin(days))].to_csv('RS_merged_at_stop_level_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ti = np.unique(merged_data['trip_id'][merged_data['opd_date'] == '2019-01-15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(unique_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_trips = list(trip_ids) + [40569091, 39684988, 40682863, 40685274, 40683341]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
