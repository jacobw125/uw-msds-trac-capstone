{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36264bitbaseconda7b74234baf454d23886bd31545e276ad",
   "display_name": "Python 3.6.2 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n3.6.2 |Anaconda custom (64-bit)| (default, Sep 21 2017, 18:29:43) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
    }
   ],
   "source": [
    "from sys import version\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from os import makedirs\n",
    "makedirs(\"final_nn\", exist_ok=True)\n",
    "\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training dimension: (482048, 21)\nXval dimension: (60253, 21)\nTest dimension: (59999, 21)\nopd_date\ntrip_start_hr_15\nrte\ndir\nday_of_week\nis_ns\nis_rapid\nis_weekend\norca_total\nfrac_disabled\nfrac_youth\nfrac_senior\nfrac_li\nfrac_uw\nons\nregion\nstart\nend\ntype\nsummer\nhour\n"
    }
   ],
   "source": [
    "# Load and prepare training and xval data\n",
    "TRAINING_FILE, XVAL_FILE, TEST_FILE = \"../combined_data/15min/train.tsv.gz\", \"../combined_data/15min/xval.tsv.gz\", \"../combined_data/15min/test.tsv.gz\"\n",
    "train, xval, test = pd.read_csv(TRAINING_FILE, sep='\\t'), pd.read_csv(XVAL_FILE, sep='\\t'), pd.read_csv(TEST_FILE, sep='\\t')\n",
    "# Remove RapidRide routes.\n",
    "train = train.loc[train['is_rapid'] == 0.,]\n",
    "train.to_csv('../combined_data/15min/train_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "xval = xval.loc[xval['is_rapid'] == 0.,]\n",
    "xval.to_csv('../combined_data/15min/xval_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "test = test.loc[test['is_rapid'] == 0.,]\n",
    "test.to_csv('../combined_data/15min/test_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "\n",
    "train['hour'] = train['trip_start_hr_15'].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "xval['hour'] = xval['trip_start_hr_15'].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "test['hour'] = test['trip_start_hr_15'].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "print(f'Training dimension: {train.shape}')\n",
    "print(f'Xval dimension: {xval.shape}')\n",
    "print(f'Test dimension: {test.shape}')\n",
    "print('\\n'.join(train.columns))\n",
    "#train.head(n=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Creating X_train\nCreating X_xval\nCreating X_test\n"
    }
   ],
   "source": [
    "NUM_COLS_TO_SCALE = [\n",
    "    'orca_total', \n",
    "    'frac_disabled', \n",
    "    'frac_youth', \n",
    "    'frac_senior', \n",
    "    'frac_li', \n",
    "    'frac_uw',\n",
    "    'hour'\n",
    "]\n",
    "NUM_COLS_OTHER = [\n",
    "    #'hour'\n",
    "]\n",
    "X_CAT_COLS = [\n",
    "    'is_ns', \n",
    "    #'is_rapid', \n",
    "    'is_weekend', \n",
    "    'trip_start_hr_15',\n",
    "    'rte', \n",
    "    'dir', \n",
    "    'day_of_week', \n",
    "    'region', \n",
    "    'start', \n",
    "    'end', \n",
    "    'summer'\n",
    "]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Creating X_train\")\n",
    "X_train = np.concatenate((\n",
    "    scaler.fit_transform(train[NUM_COLS_TO_SCALE]),\n",
    "    train[NUM_COLS_OTHER],\n",
    "    one_hot_encoder.fit_transform(train[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_xval\")\n",
    "X_xval = np.concatenate((\n",
    "    scaler.transform(xval[NUM_COLS_TO_SCALE]),\n",
    "    xval[NUM_COLS_OTHER],\n",
    "    one_hot_encoder.transform(xval[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_test\")\n",
    "X_test = np.concatenate((\n",
    "    scaler.transform(test[NUM_COLS_TO_SCALE]),\n",
    "    test[NUM_COLS_OTHER],\n",
    "    one_hot_encoder.transform(test[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "y_train = train['ons']\n",
    "y_xval = xval['ons']\n",
    "y_test = test['ons']\n",
    "\n",
    "np.save(\"final_nn/preprocessed_15m_X_train.npy\", X_train)\n",
    "np.save(\"final_nn/preprocessed_15m_X_xval.npy\", X_xval)\n",
    "np.save(\"final_nn/preprocessed_15m_X_test.npy\", X_test)\n",
    "np.save(\"final_nn/y_15m_train.npy\", y_train)\n",
    "np.save(\"final_nn/y_15m_xval.npy\", y_xval)\n",
    "np.save(\"final_nn/y_15m_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(482048, 414)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['Scaled: orca_total', 'Scaled: frac_disabled', 'Scaled: frac_youth', 'Scaled: frac_senior', 'Scaled: frac_li', 'Scaled: frac_uw', 'Scaled: hour', 'is_ns: 0.0', 'is_ns: 1.0', 'is_weekend: 0.0', 'is_weekend: 1.0', 'trip_start_hr_15: 00_0', 'trip_start_hr_15: 00_15', 'trip_start_hr_15: 00_30', 'trip_start_hr_15: 00_45', 'trip_start_hr_15: 01_0', 'trip_start_hr_15: 01_15', 'trip_start_hr_15: 01_30', 'trip_start_hr_15: 01_45', 'trip_start_hr_15: 02_0', 'trip_start_hr_15: 02_15', 'trip_start_hr_15: 02_30', 'trip_start_hr_15: 02_45', 'trip_start_hr_15: 03_0', 'trip_start_hr_15: 03_15', 'trip_start_hr_15: 03_30', 'trip_start_hr_15: 03_45', 'trip_start_hr_15: 04_0', 'trip_start_hr_15: 04_15', 'trip_start_hr_15: 04_30', 'trip_start_hr_15: 04_45', 'trip_start_hr_15: 05_0', 'trip_start_hr_15: 05_15', 'trip_start_hr_15: 05_30', 'trip_start_hr_15: 05_45', 'trip_start_hr_15: 06_0', 'trip_start_hr_15: 06_15', 'trip_start_hr_15: 06_30', 'trip_start_hr_15: 06_45', 'trip_start_hr_15: 07_0', 'trip_start_hr_15: 07_15', 'trip_start_hr_15: 07_30', 'trip_start_hr_15: 07_45', 'trip_start_hr_15: 08_0', 'trip_start_hr_15: 08_15', 'trip_start_hr_15: 08_30', 'trip_start_hr_15: 08_45', 'trip_start_hr_15: 09_0', 'trip_start_hr_15: 09_15', 'trip_start_hr_15: 09_30', 'trip_start_hr_15: 09_45', 'trip_start_hr_15: 10_0', 'trip_start_hr_15: 10_15', 'trip_start_hr_15: 10_30', 'trip_start_hr_15: 10_45', 'trip_start_hr_15: 11_0', 'trip_start_hr_15: 11_15', 'trip_start_hr_15: 11_30', 'trip_start_hr_15: 11_45', 'trip_start_hr_15: 12_0', 'trip_start_hr_15: 12_15', 'trip_start_hr_15: 12_30', 'trip_start_hr_15: 12_45', 'trip_start_hr_15: 13_0', 'trip_start_hr_15: 13_15', 'trip_start_hr_15: 13_30', 'trip_start_hr_15: 13_45', 'trip_start_hr_15: 14_0', 'trip_start_hr_15: 14_15', 'trip_start_hr_15: 14_30', 'trip_start_hr_15: 14_45', 'trip_start_hr_15: 15_0', 'trip_start_hr_15: 15_15', 'trip_start_hr_15: 15_30', 'trip_start_hr_15: 15_45', 'trip_start_hr_15: 16_0', 'trip_start_hr_15: 16_15', 'trip_start_hr_15: 16_30', 'trip_start_hr_15: 16_45', 'trip_start_hr_15: 17_0', 'trip_start_hr_15: 17_15', 'trip_start_hr_15: 17_30', 'trip_start_hr_15: 17_45', 'trip_start_hr_15: 18_0', 'trip_start_hr_15: 18_15', 'trip_start_hr_15: 18_30', 'trip_start_hr_15: 18_45', 'trip_start_hr_15: 19_0', 'trip_start_hr_15: 19_15', 'trip_start_hr_15: 19_30', 'trip_start_hr_15: 19_45', 'trip_start_hr_15: 20_0', 'trip_start_hr_15: 20_15', 'trip_start_hr_15: 20_30', 'trip_start_hr_15: 20_45', 'trip_start_hr_15: 21_0', 'trip_start_hr_15: 21_15', 'trip_start_hr_15: 21_30', 'trip_start_hr_15: 21_45', 'trip_start_hr_15: 22_0', 'trip_start_hr_15: 22_15', 'trip_start_hr_15: 22_30', 'trip_start_hr_15: 22_45', 'trip_start_hr_15: 23_0', 'trip_start_hr_15: 23_15', 'trip_start_hr_15: 23_30', 'trip_start_hr_15: 23_45', 'rte: 1', 'rte: 2', 'rte: 3', 'rte: 4', 'rte: 5', 'rte: 7', 'rte: 8', 'rte: 9', 'rte: 10', 'rte: 11', 'rte: 12', 'rte: 13', 'rte: 14', 'rte: 15', 'rte: 17', 'rte: 18', 'rte: 19', 'rte: 21', 'rte: 22', 'rte: 24', 'rte: 26', 'rte: 27', 'rte: 28', 'rte: 29', 'rte: 31', 'rte: 32', 'rte: 33', 'rte: 36', 'rte: 37', 'rte: 40', 'rte: 41', 'rte: 43', 'rte: 44', 'rte: 45', 'rte: 47', 'rte: 48', 'rte: 49', 'rte: 50', 'rte: 55', 'rte: 56', 'rte: 57', 'rte: 60', 'rte: 62', 'rte: 63', 'rte: 64', 'rte: 65', 'rte: 67', 'rte: 70', 'rte: 71', 'rte: 73', 'rte: 74', 'rte: 75', 'rte: 76', 'rte: 77', 'rte: 78', 'rte: 101', 'rte: 102', 'rte: 105', 'rte: 106', 'rte: 107', 'rte: 111', 'rte: 113', 'rte: 114', 'rte: 116', 'rte: 118', 'rte: 119', 'rte: 120', 'rte: 121', 'rte: 122', 'rte: 123', 'rte: 124', 'rte: 125', 'rte: 128', 'rte: 131', 'rte: 132', 'rte: 143', 'rte: 148', 'rte: 150', 'rte: 153', 'rte: 154', 'rte: 156', 'rte: 157', 'rte: 158', 'rte: 159', 'rte: 164', 'rte: 166', 'rte: 167', 'rte: 168', 'rte: 169', 'rte: 177', 'rte: 178', 'rte: 179', 'rte: 180', 'rte: 181', 'rte: 182', 'rte: 183', 'rte: 186', 'rte: 187', 'rte: 190', 'rte: 192', 'rte: 193', 'rte: 197', 'rte: 200', 'rte: 201', 'rte: 204', 'rte: 208', 'rte: 212', 'rte: 214', 'rte: 216', 'rte: 217', 'rte: 218', 'rte: 219', 'rte: 221', 'rte: 224', 'rte: 226', 'rte: 232', 'rte: 234', 'rte: 235', 'rte: 236', 'rte: 237', 'rte: 238', 'rte: 240', 'rte: 241', 'rte: 243', 'rte: 244', 'rte: 245', 'rte: 246', 'rte: 248', 'rte: 249', 'rte: 252', 'rte: 255', 'rte: 257', 'rte: 268', 'rte: 269', 'rte: 271', 'rte: 277', 'rte: 301', 'rte: 303', 'rte: 304', 'rte: 308', 'rte: 309', 'rte: 311', 'rte: 312', 'rte: 316', 'rte: 330', 'rte: 331', 'rte: 342', 'rte: 345', 'rte: 346', 'rte: 347', 'rte: 348', 'rte: 355', 'rte: 372', 'rte: 373', 'rte: 522', 'rte: 540', 'rte: 541', 'rte: 542', 'rte: 545', 'rte: 550', 'rte: 554', 'rte: 555', 'rte: 556', 'dir: E', 'dir: N', 'dir: S', 'dir: W', 'day_of_week: 0', 'day_of_week: 1', 'day_of_week: 2', 'day_of_week: 3', 'day_of_week: 4', 'day_of_week: 5', 'day_of_week: 6', 'region: East King County', 'region: Express', 'region: North King County', 'region: Seattle', 'region: South King County', 'start: Admiral District', 'start: Alaska Junction', 'start: Alki Point', 'start: Auburn Station', 'start: Ballard', 'start: Beacon Hill station', 'start: Bellevue', 'start: Blue Ridge', 'start: Brickyard P&R', 'start: Burien Transit Center', 'start: Capitol Hill', 'start: Carkeek Park', 'start: Central Magnolia', \"start: Children's Hospital\", 'start: Downtown Seattle', 'start: Duvall', 'start: East Magnolia', 'start: Eastgate P&R', 'start: Federal Center South', 'start: Federal Way', 'start: First Hill', 'start: Green Lake', 'start: Highline College', 'start: International District/Chinatown station', 'start: Issaquah', 'start: Jackson Park', 'start: Juanita', 'start: Kenmore', 'start: Kent Station', 'start: Kingsgate', 'start: Kinnear', 'start: Kirkland Transit Center', 'start: Lake City', 'start: Lake Meridian P&R', 'start: Loyal Heights', 'start: North Beach', 'start: North City', 'start: North Mercer Island', 'start: Northgate Transit Center', 'start: Overlake', 'start: Renton', 'start: Sand Point', 'start: Seattle Center', 'start: Seattle Pacific University', 'start: Shoreline', 'start: Sunset Hill', 'start: Twin Lakes', 'start: University District', 'start: Unknown', 'start: Vashon Island Ferry Terminal', 'start: Wedgwood', 'start: West Magnolia', 'start: West Queen Anne', 'start: West Seattle', 'end: Arbor Heights', 'end: Aurora Village Transit Center', 'end: Avondale', 'end: Bear Creek P&R', 'end: Bellevue', 'end: Black Diamond', 'end: Bothell', 'end: Burien Transit Center', 'end: Capitol Hill', 'end: Cherry Hill', \"end: Children's Hospital\", 'end: Clyde Hill', 'end: Colman Park', 'end: Dockton', 'end: Downtown Seattle', 'end: Eastgate P&R', 'end: Education Hill', 'end: Enumclaw', 'end: Factoria', 'end: Fairwood', 'end: Fauntleroy Ferry Terminal', 'end: Federal Way', 'end: Green River College', 'end: Highline College', 'end: Horizon View', 'end: Interlaken Park', 'end: Issaquah', 'end: Judkins Park', 'end: Kenmore', 'end: Kent Station', 'end: Kingsgate', 'end: Kirkland Transit Center', 'end: Lake Kathleen', 'end: Lake Meridian P&R', 'end: Madison Park', 'end: Madrona', 'end: Maple Valley', 'end: Meridian Park', 'end: Mount Baker', 'end: Mountlake Terrace Transit Center', 'end: North Bend', 'end: North Issaquah', 'end: Northeast Tacoma (crosses into Pierce County)', 'end: Othello Station', 'end: Overlake', 'end: Rainier Beach', 'end: Rainier Beach station', 'end: Redmond', 'end: Redondo Heights P&R', 'end: Renton', 'end: Richmond Beach', 'end: Sand Point', 'end: Seahurst', 'end: Shoreline', 'end: Shorewood', 'end: South Bellevue P&R', 'end: South Federal Way P&R', 'end: South Mercer Island', 'end: South Renton P&R', 'end: Southcenter', 'end: Southeast Auburn', 'end: Star Lake P&R', 'end: Summit', 'end: Swedish Medical Center Issaquah', 'end: Tahlequah Ferry Terminal', 'end: Timberlane', 'end: Tukwila', 'end: Twin Lakes', 'end: University District', 'end: Unknown', 'end: Westwood Village', 'end: Woodinville', 'summer: False', 'summer: True']\n"
    }
   ],
   "source": [
    "one_hot_encoder.categories_\n",
    "\n",
    "column_labels = list()\n",
    "for lab in NUM_COLS_TO_SCALE:\n",
    "    column_labels.append(f'Scaled: {lab}')\n",
    "\n",
    "for lab in NUM_COLS_OTHER:\n",
    "    column_labels.append(f'Numeric: {lab}')\n",
    "\n",
    "for i, cat in enumerate(X_CAT_COLS):\n",
    "    for cat_val in one_hot_encoder.categories_[i]:\n",
    "        column_labels.append(f'{cat}: {cat_val}')\n",
    "\n",
    "assert len(column_labels) == X_train.shape[1], f\"Len of column labels {len(column_labels)} matches dimension of training set {X_train.shape}\"\n",
    "\n",
    "import pickle\n",
    "with open('final_nn/15m_one_hot_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(one_hot_encoder, f)\n",
    "\n",
    "with open('final_nn/15m_standard_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('final_nn/15m_column_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(column_labels, f)\n",
    "\n",
    "print(column_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting model with an initial layer of width 820, dropout=0.2253151264543593 and lr=0.9329157092815854\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 49s 101us/step - loss: 8.2167 - val_loss: 6.6112\nEpoch 2/25\n482048/482048 [==============================] - 49s 101us/step - loss: 7.0947 - val_loss: 6.1950\nEpoch 3/25\n482048/482048 [==============================] - 49s 102us/step - loss: 6.8251 - val_loss: 6.2546\nRestoring model weights from the end of the best epoch\nEpoch 00003: early stopping\n60253/60253 [==============================] - 4s 65us/step\nFitting model with an initial layer of width 1053, dropout=0.2476364041855147 and lr=0.654224356418054\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 57s 118us/step - loss: 7.6382 - val_loss: 6.1986\nEpoch 2/25\n482048/482048 [==============================] - 58s 119us/step - loss: 6.5592 - val_loss: 6.1296\nEpoch 3/25\n482048/482048 [==============================] - 56s 116us/step - loss: 6.3605 - val_loss: 5.9149\nEpoch 4/25\n482048/482048 [==============================] - 64s 132us/step - loss: 6.2404 - val_loss: 5.7479\nEpoch 5/25\n482048/482048 [==============================] - 58s 120us/step - loss: 6.1595 - val_loss: 5.8713\nRestoring model weights from the end of the best epoch\nEpoch 00005: early stopping\n60253/60253 [==============================] - 5s 85us/step\nFitting model with an initial layer of width 235, dropout=0.187621206322808 and lr=0.7754862394618256\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 26s 53us/step - loss: 7.3468 - val_loss: 6.1636\nEpoch 2/25\n482048/482048 [==============================] - 24s 51us/step - loss: 6.7377 - val_loss: 6.0489\nEpoch 3/25\n482048/482048 [==============================] - 24s 51us/step - loss: 6.5919 - val_loss: 6.0014\nEpoch 4/25\n482048/482048 [==============================] - 25s 51us/step - loss: 6.5345 - val_loss: 6.0386\nRestoring model weights from the end of the best epoch\nEpoch 00004: early stopping\n60253/60253 [==============================] - 3s 47us/step\nFitting model with an initial layer of width 1050, dropout=0.17894449552976913 and lr=0.664744581560279\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 58s 121us/step - loss: 7.5829 - val_loss: 6.2955\nEpoch 2/25\n482048/482048 [==============================] - 58s 120us/step - loss: 6.5836 - val_loss: 6.7570\nRestoring model weights from the end of the best epoch\nEpoch 00002: early stopping\n60253/60253 [==============================] - 4s 72us/step\nFitting model with an initial layer of width 1052, dropout=0.24621861069424075 and lr=0.13644638198737824\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 57s 118us/step - loss: 7.7508 - val_loss: 6.6439\nEpoch 2/25\n482048/482048 [==============================] - 56s 116us/step - loss: 6.5976 - val_loss: 6.1878\nEpoch 3/25\n482048/482048 [==============================] - 57s 119us/step - loss: 6.3033 - val_loss: 5.9674\nEpoch 4/25\n482048/482048 [==============================] - 57s 118us/step - loss: 6.1445 - val_loss: 5.8335\nEpoch 5/25\n482048/482048 [==============================] - 61s 127us/step - loss: 6.0307 - val_loss: 5.7210\nEpoch 6/25\n482048/482048 [==============================] - 59s 123us/step - loss: 5.9745 - val_loss: 5.7785\nRestoring model weights from the end of the best epoch\nEpoch 00006: early stopping\n60253/60253 [==============================] - 4s 74us/step\nFitting model with an initial layer of width 1020, dropout=0.23544620354920223 and lr=0.16287174712352556\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 61s 126us/step - loss: 7.6297 - val_loss: 6.4917\nEpoch 2/25\n482048/482048 [==============================] - 57s 119us/step - loss: 6.5225 - val_loss: 6.1134\nEpoch 3/25\n482048/482048 [==============================] - 60s 124us/step - loss: 6.2412 - val_loss: 5.9204\nEpoch 4/25\n482048/482048 [==============================] - 62s 128us/step - loss: 6.0975 - val_loss: 5.8308\nEpoch 5/25\n482048/482048 [==============================] - 57s 118us/step - loss: 6.0037 - val_loss: 5.7809\nEpoch 6/25\n482048/482048 [==============================] - 55s 114us/step - loss: 5.9448 - val_loss: 5.7121\nEpoch 7/25\n482048/482048 [==============================] - 56s 117us/step - loss: 5.8948 - val_loss: 5.6641\nEpoch 8/25\n482048/482048 [==============================] - 58s 120us/step - loss: 5.8504 - val_loss: 5.6123\nEpoch 9/25\n482048/482048 [==============================] - 64s 133us/step - loss: 5.8260 - val_loss: 5.5478\nEpoch 10/25\n482048/482048 [==============================] - 57s 119us/step - loss: 5.7963 - val_loss: 5.5389\nEpoch 11/25\n482048/482048 [==============================] - 57s 118us/step - loss: 5.7757 - val_loss: 5.5070\nEpoch 12/25\n482048/482048 [==============================] - 56s 117us/step - loss: 5.7436 - val_loss: 5.5462\nRestoring model weights from the end of the best epoch\nEpoch 00012: early stopping\n60253/60253 [==============================] - 5s 88us/step\nFitting model with an initial layer of width 1049, dropout=0.22291943431817002 and lr=0.6840137572314847\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 64s 132us/step - loss: 7.7880 - val_loss: 7.1279\nEpoch 2/25\n482048/482048 [==============================] - 67s 140us/step - loss: 6.6584 - val_loss: 6.2989\nEpoch 3/25\n482048/482048 [==============================] - 62s 129us/step - loss: 6.4349 - val_loss: 5.8923\nEpoch 4/25\n482048/482048 [==============================] - 62s 129us/step - loss: 6.3124 - val_loss: 5.8650\nEpoch 5/25\n482048/482048 [==============================] - 67s 139us/step - loss: 6.2026 - val_loss: 5.7893\nEpoch 6/25\n482048/482048 [==============================] - 73s 152us/step - loss: 6.1324 - val_loss: 5.8980\nRestoring model weights from the end of the best epoch\nEpoch 00006: early stopping\n60253/60253 [==============================] - 5s 79us/step\nFitting model with an initial layer of width 972, dropout=0.24325000998332277 and lr=0.3135297010469077\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 66s 137us/step - loss: 7.2862 - val_loss: 6.2432\nEpoch 2/25\n482048/482048 [==============================] - 63s 130us/step - loss: 6.3513 - val_loss: 5.9443\nEpoch 3/25\n482048/482048 [==============================] - 58s 121us/step - loss: 6.1441 - val_loss: 5.7646\nEpoch 4/25\n482048/482048 [==============================] - 50s 104us/step - loss: 6.0354 - val_loss: 5.7113\nEpoch 5/25\n482048/482048 [==============================] - 50s 103us/step - loss: 5.9674 - val_loss: 5.7542\nRestoring model weights from the end of the best epoch\nEpoch 00005: early stopping\n60253/60253 [==============================] - 4s 66us/step\nFitting model with an initial layer of width 1014, dropout=0.23714722666753235 and lr=0.3201151459589046\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 53s 109us/step - loss: 7.2920 - val_loss: 6.2410\nEpoch 2/25\n482048/482048 [==============================] - 51s 107us/step - loss: 6.3332 - val_loss: 5.8757\nEpoch 3/25\n482048/482048 [==============================] - 51s 107us/step - loss: 6.1296 - val_loss: 5.9041\nRestoring model weights from the end of the best epoch\nEpoch 00003: early stopping\n60253/60253 [==============================] - 4s 66us/step\nFitting model with an initial layer of width 1018, dropout=0.21658849489409773 and lr=0.1501056095638651\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 53s 109us/step - loss: 7.6475 - val_loss: 6.5417\nEpoch 2/25\n482048/482048 [==============================] - 52s 108us/step - loss: 6.5067 - val_loss: 6.1114\nEpoch 3/25\n482048/482048 [==============================] - 52s 107us/step - loss: 6.2195 - val_loss: 5.8618\nEpoch 4/25\n482048/482048 [==============================] - 51s 107us/step - loss: 6.0584 - val_loss: 5.8036\nEpoch 5/25\n482048/482048 [==============================] - 52s 107us/step - loss: 5.9735 - val_loss: 5.7287\nEpoch 6/25\n482048/482048 [==============================] - 51s 107us/step - loss: 5.9030 - val_loss: 5.6653\nEpoch 7/25\n482048/482048 [==============================] - 57s 118us/step - loss: 5.8556 - val_loss: 5.6012\nEpoch 8/25\n482048/482048 [==============================] - 60s 125us/step - loss: 5.8232 - val_loss: 5.5750\nEpoch 9/25\n482048/482048 [==============================] - 61s 126us/step - loss: 5.7917 - val_loss: 5.5152\nEpoch 10/25\n482048/482048 [==============================] - 60s 124us/step - loss: 5.7625 - val_loss: 5.4893\nEpoch 11/25\n482048/482048 [==============================] - 59s 122us/step - loss: 5.7490 - val_loss: 5.5549\nRestoring model weights from the end of the best epoch\nEpoch 00011: early stopping\n60253/60253 [==============================] - 5s 78us/step\nFitting model with an initial layer of width 1027, dropout=0.20743465782607176 and lr=0.419646354303326\nTrain on 482048 samples, validate on 60253 samples\nEpoch 1/25\n161792/482048 [=========>....................] - ETA: 40s - loss: 8.6582"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-30c648da6858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     ],\n\u001b[1;32m     23\u001b[0m     \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mn_random_starts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         callback=callback, n_jobs=n_jobs, model_queue_size=model_queue_size)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-30c648da6858>\u001b[0m in \u001b[0;36mmodel_fit\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_xval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_xval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find the best possible model using the xval data\n",
    "from skopt import gp_minimize\n",
    "\n",
    "def model_fit(params):   # size=600, dropout=0.1, lr=0.2, max_iter=25\n",
    "    size, dropout, lr = params\n",
    "    max_iter = 25\n",
    "    print(f\"Fitting model with an initial layer of width {size}, dropout={dropout} and lr={lr}\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, activation=\"sigmoid\", input_dim=X_train.shape[1]))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=max_iter, batch_size=256, callbacks=[es])\n",
    "    return model.evaluate(X_xval, y_xval)\n",
    "\n",
    "best = gp_minimize(model_fit, [\n",
    "        (200, 1200),  # size\n",
    "        (0.1, 0.5),   # dropout\n",
    "        (0.1, 1)      # lr\n",
    "    ],\n",
    "    n_calls=25,\n",
    "    n_random_starts=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 482048 samples, validate on 60253 samples\nEpoch 1/25\n482048/482048 [==============================] - 58s 119us/step - loss: 7.6116 - val_loss: 6.4744\nEpoch 2/25\n482048/482048 [==============================] - 56s 115us/step - loss: 6.4908 - val_loss: 6.2425\nEpoch 3/25\n482048/482048 [==============================] - 52s 108us/step - loss: 6.2136 - val_loss: 5.9749\nEpoch 4/25\n482048/482048 [==============================] - 52s 109us/step - loss: 6.0621 - val_loss: 5.8031\nEpoch 5/25\n482048/482048 [==============================] - 55s 113us/step - loss: 5.9765 - val_loss: 5.6810\nEpoch 6/25\n482048/482048 [==============================] - 56s 116us/step - loss: 5.9208 - val_loss: 5.6519\nEpoch 7/25\n482048/482048 [==============================] - 56s 116us/step - loss: 5.8820 - val_loss: 5.6024\nEpoch 8/25\n482048/482048 [==============================] - 53s 110us/step - loss: 5.8401 - val_loss: 5.5940\nEpoch 9/25\n482048/482048 [==============================] - 53s 109us/step - loss: 5.8114 - val_loss: 5.5896\nEpoch 10/25\n482048/482048 [==============================] - 52s 108us/step - loss: 5.7941 - val_loss: 5.5301\nEpoch 11/25\n482048/482048 [==============================] - 53s 111us/step - loss: 5.7629 - val_loss: 5.5221\nEpoch 12/25\n482048/482048 [==============================] - 56s 116us/step - loss: 5.7498 - val_loss: 5.4969\nEpoch 13/25\n482048/482048 [==============================] - 56s 116us/step - loss: 5.7295 - val_loss: 5.5074\nRestoring model weights from the end of the best epoch\nEpoch 00013: early stopping\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1020, activation=\"sigmoid\", input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.23544620354920223))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.16287174712352556, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=25, batch_size=256, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (378,) but got array with shape (367,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a275c5ed06d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test perf: {model.evaluate(X_test, y_test, batch_size=128)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (378,) but got array with shape (367,)"
     ]
    }
   ],
   "source": [
    "print(f'Test perf: {model.evaluate(X_test, y_test, batch_size=128)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"final_nn/model_15min.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"final_nn/model_15min_weights_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create test set predictions\n",
    "# from keras.models import model_from_json\n",
    "\n",
    "# with open(\"final_nn/model_15min.json\", \"r\") as json_file:\n",
    "#     model = model_from_json(json_file.read())\n",
    "\n",
    "# model.load_weights(\"final_nn/model_15min_weights_train.h5\")\n",
    "\n",
    "with open(\"../predictions/final_nn_15min_test.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_test).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')\n",
    "\n",
    "with open(\"../predictions/final_nn_15min_xval.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_xval).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Creating X_train\nCreating X_xval\nCreating X_test\n"
    }
   ],
   "source": [
    "# Fit another model on 30m data.\n",
    "# Load and prepare training and xval data\n",
    "TRAINING_FILE, XVAL_FILE, TEST_FILE = \"../combined_data/30min/train.tsv.gz\", \"../combined_data/30min/xval.tsv.gz\", \"../combined_data/30min/test.tsv.gz\"\n",
    "train, xval, test = pd.read_csv(TRAINING_FILE, sep='\\t'), pd.read_csv(XVAL_FILE, sep='\\t'), pd.read_csv(TEST_FILE, sep='\\t')\n",
    "# Remove RapidRide routes.\n",
    "train = train.loc[train['is_rapid'] == 0.,]\n",
    "train.to_csv('../combined_data/30min/train_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "xval = xval.loc[xval['is_rapid'] == 0.,]\n",
    "xval.to_csv('../combined_data/30min/xval_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "test = test.loc[test['is_rapid'] == 0.,]\n",
    "test.to_csv('../combined_data/30min/test_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "\n",
    "train['hour'] = train['trip_start_hr_30'].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "xval['hour'] = xval['trip_start_hr_30'].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "test['hour'] = test['trip_start_hr_30'].apply(lambda x: int(x.split(\"_\")[0]))\n",
    "\n",
    "X_NUM_COLS = [\n",
    "    'orca_total', \n",
    "    'frac_disabled', \n",
    "    'frac_youth', \n",
    "    'frac_senior', \n",
    "    'frac_li', \n",
    "    'frac_uw',\n",
    "    'hour'\n",
    "]\n",
    "X_CAT_COLS = [\n",
    "    'is_ns', \n",
    "    'is_rapid', \n",
    "    'is_weekend', \n",
    "    'trip_start_hr_30',\n",
    "    'rte', \n",
    "    'dir', \n",
    "    'day_of_week', \n",
    "    'region', \n",
    "    'start', \n",
    "    'end', \n",
    "    'summer'\n",
    "]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Creating X_train\")\n",
    "X_train = np.concatenate((\n",
    "    scaler.fit_transform(train[X_NUM_COLS]),\n",
    "    one_hot_encoder.fit_transform(train[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_xval\")\n",
    "X_xval = np.concatenate((\n",
    "    scaler.transform(xval[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(xval[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_test\")\n",
    "X_test = np.concatenate((\n",
    "    scaler.transform(test[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(test[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "y_train = train['ons']\n",
    "y_xval = xval['ons']\n",
    "y_test = test['ons']\n",
    "\n",
    "np.save(\"final_nn/preprocessed_30m_X_train.npy\", X_train)\n",
    "np.save(\"final_nn/preprocessed_30m_X_xval.npy\", X_xval)\n",
    "np.save(\"final_nn/preprocessed_30m_X_test.npy\", X_test)\n",
    "np.save(\"final_nn/y_30m_train.npy\", y_train)\n",
    "np.save(\"final_nn/y_30m_xval.npy\", y_xval)\n",
    "np.save(\"final_nn/y_30m_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = list()\n",
    "for lab in X_NUM_COLS:\n",
    "    column_labels.append(f'num: {lab}')\n",
    "\n",
    "for i, cat in enumerate(X_CAT_COLS):\n",
    "    for cat_val in one_hot_encoder.categories_[i]:\n",
    "        column_labels.append(f'{cat}: {cat_val}')\n",
    "\n",
    "assert len(column_labels) == X_train.shape[1], f\"Len of column labels {len(column_labels)} matches dimension of training set {X_train.shape}\"\n",
    "\n",
    "import pickle\n",
    "with open('final_nn/30m_one_hot_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(one_hot_encoder, f)\n",
    "\n",
    "with open('final_nn/30m_standard_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('final_nn/30m_column_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(column_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting model with an initial layer of width 914, dropout=0.45775133977570537 and lr=0.37312365193316577\nTrain on 385130 samples, validate on 48131 samples\nEpoch 1/25\n385130/385130 [==============================] - 43s 113us/step - loss: 9.6141 - val_loss: 7.9202\nEpoch 2/25\n385130/385130 [==============================] - 41s 106us/step - loss: 8.3410 - val_loss: 7.8579\nEpoch 3/25\n385130/385130 [==============================] - 38s 99us/step - loss: 8.0297 - val_loss: 7.5099\nEpoch 4/25\n385130/385130 [==============================] - 36s 94us/step - loss: 7.8929 - val_loss: 7.1742\nEpoch 5/25\n385130/385130 [==============================] - 37s 97us/step - loss: 7.7723 - val_loss: 7.4899\nRestoring model weights from the end of the best epoch\nEpoch 00005: early stopping\n48131/48131 [==============================] - 3s 63us/step\nFitting model with an initial layer of width 937, dropout=0.2086839863750497 and lr=0.7672984471122088\nTrain on 385130 samples, validate on 48131 samples\nEpoch 1/25\n385130/385130 [==============================] - 40s 103us/step - loss: 9.8813 - val_loss: 9.2993\nEpoch 2/25\n385130/385130 [==============================] - 39s 100us/step - loss: 8.4778 - val_loss: 7.7648\nEpoch 3/25\n385130/385130 [==============================] - 44s 115us/step - loss: 8.1713 - val_loss: 7.3415\nEpoch 4/25\n385130/385130 [==============================] - 39s 102us/step - loss: 8.0240 - val_loss: 8.4240\nRestoring model weights from the end of the best epoch\nEpoch 00004: early stopping\n48131/48131 [==============================] - 3s 65us/step\nFitting model with an initial layer of width 918, dropout=0.2689648745578809 and lr=0.6276661243214614\nTrain on 385130 samples, validate on 48131 samples\nEpoch 1/25\n385130/385130 [==============================] - 40s 104us/step - loss: 9.6562 - val_loss: 8.2680\nEpoch 2/25\n385130/385130 [==============================] - 53s 138us/step - loss: 8.4343 - val_loss: 8.1223\nEpoch 3/25\n385130/385130 [==============================] - 50s 131us/step - loss: 8.1541 - val_loss: 7.2935\nEpoch 4/25\n385130/385130 [==============================] - 52s 135us/step - loss: 7.9695 - val_loss: 7.2940\nRestoring model weights from the end of the best epoch\nEpoch 00004: early stopping\n48131/48131 [==============================] - 5s 101us/step\nFitting model with an initial layer of width 900, dropout=0.5 and lr=0.20096727957072583\nTrain on 385130 samples, validate on 48131 samples\nEpoch 1/25\n 81664/385130 [=====>........................] - ETA: 45s - loss: 12.0980"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9b84d370e386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     ],\n\u001b[1;32m     23\u001b[0m     \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mn_random_starts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         callback=callback, n_jobs=n_jobs, model_queue_size=model_queue_size)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-9b84d370e386>\u001b[0m in \u001b[0;36mmodel_fit\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_xval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_xval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find the best possible model using the xval data\n",
    "from skopt import gp_minimize\n",
    "\n",
    "def model_fit(params):\n",
    "    size, dropout, lr = params\n",
    "    max_iter = 25\n",
    "    print(f\"Fitting model with an initial layer of width {size}, dropout={dropout} and lr={lr}\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, activation=\"sigmoid\", input_dim=X_train.shape[1]))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=max_iter, batch_size=256, callbacks=[es])\n",
    "    return model.evaluate(X_xval, y_xval)\n",
    "\n",
    "best = gp_minimize(model_fit, [\n",
    "        (800, 2000),  # size\n",
    "        (0.1, 0.5),   # dropout\n",
    "        (0.1, 1)      # lr\n",
    "    ],\n",
    "    n_calls=25,\n",
    "    n_random_starts=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From /Users/jwarwick/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From /Users/jwarwick/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nWARNING:tensorflow:From /Users/jwarwick/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nTrain on 385130 samples, validate on 48131 samples\nEpoch 1/25\nWARNING:tensorflow:From /Users/jwarwick/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From /Users/jwarwick/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /Users/jwarwick/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /Users/jwarwick/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From /Users/jwarwick/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\n385130/385130 [==============================] - 85s 220us/step - loss: 10.4403 - val_loss: 9.2446\nEpoch 2/25\n385130/385130 [==============================] - 77s 200us/step - loss: 8.5830 - val_loss: 8.2695\nEpoch 3/25\n385130/385130 [==============================] - 72s 187us/step - loss: 8.0249 - val_loss: 7.6752\nEpoch 4/25\n385130/385130 [==============================] - 71s 183us/step - loss: 7.6877 - val_loss: 7.4072\nEpoch 5/25\n385130/385130 [==============================] - 83s 216us/step - loss: 7.4562 - val_loss: 7.1782\nEpoch 6/25\n385130/385130 [==============================] - 77s 201us/step - loss: 7.2813 - val_loss: 7.0189\nEpoch 7/25\n385130/385130 [==============================] - 76s 198us/step - loss: 7.1571 - val_loss: 7.0781\nRestoring model weights from the end of the best epoch\nEpoch 00007: early stopping\n48169/48169 [==============================] - 6s 120us/step\n"
    },
    {
     "data": {
      "text/plain": "6.991281128495925"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After running the above step on cloud compute, the best model I found was this:\n",
    "# 2000, dropout=0.1 and lr=0.1\n",
    "model = Sequential()\n",
    "model.add(Dense(2000, activation=\"sigmoid\", input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=25, batch_size=256, callbacks=[es])\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"final_nn/model_30min.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"final_nn/model_30min_weights_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set predictions out of the better performing model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#with open(\"final_nn/model_30min.json\", \"r\") as json_file:\n",
    "#    model = model_from_json(json_file.read())\n",
    "\n",
    "#model.load_weights(\"final_nn/model_30min_weights_train.h5\")\n",
    "\n",
    "with open(\"../predictions/final_nn_30min_test.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_test).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')\n",
    "\n",
    "with open(\"../predictions/final_nn_30min_xval.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_xval).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training dimension: (250334, 21)\nXval dimension: (31572, 21)\nTest dimension: (31534, 21)\nCreating X_train\nCreating X_xval\nCreating X_test\n"
    }
   ],
   "source": [
    "# And for 1hr\n",
    "\n",
    "TRAINING_FILE, XVAL_FILE, TEST_FILE = \"../combined_data/hr/train.tsv.gz\", \"../combined_data/hr/xval.tsv.gz\", \"../combined_data/hr/test.tsv.gz\"\n",
    "train, xval, test = pd.read_csv(TRAINING_FILE, sep='\\t'), pd.read_csv(XVAL_FILE, sep='\\t'), pd.read_csv(TEST_FILE, sep='\\t')\n",
    "# Remove RapidRide routes.\n",
    "train = train.loc[train['is_rapid'] == 0.,]\n",
    "train.to_csv('../combined_data/hr/train_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "xval = xval.loc[xval['is_rapid'] == 0.,]\n",
    "xval.to_csv('../combined_data/hr/xval_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "test = test.loc[test['is_rapid'] == 0.,]\n",
    "test.to_csv('../combined_data/hr/test_no_rr.tsv.gz', sep='\\t', index=False)\n",
    "\n",
    "train['hour'] = train['trip_start_hr'].apply(int)\n",
    "xval['hour'] = xval['trip_start_hr'].apply(int)\n",
    "test['hour'] = test['trip_start_hr'].apply(int)\n",
    "print(f'Training dimension: {train.shape}')\n",
    "print(f'Xval dimension: {xval.shape}')\n",
    "print(f'Test dimension: {test.shape}')\n",
    "\n",
    "X_NUM_COLS = [\n",
    "    'orca_total', \n",
    "    'frac_disabled', \n",
    "    'frac_youth', \n",
    "    'frac_senior', \n",
    "    'frac_li', \n",
    "    'frac_uw',\n",
    "    'hour'\n",
    "]\n",
    "X_CAT_COLS = [\n",
    "    'is_ns', \n",
    "    'is_rapid', \n",
    "    'is_weekend', \n",
    "    'trip_start_hr',\n",
    "    'rte', \n",
    "    'dir', \n",
    "    'day_of_week', \n",
    "    'region', \n",
    "    'start', \n",
    "    'end', \n",
    "    'summer'\n",
    "]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Creating X_train\")\n",
    "X_train = np.concatenate((\n",
    "    scaler.fit_transform(train[X_NUM_COLS]),\n",
    "    one_hot_encoder.fit_transform(train[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_xval\")\n",
    "X_xval = np.concatenate((\n",
    "    scaler.transform(xval[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(xval[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_test\")\n",
    "X_test = np.concatenate((\n",
    "    scaler.transform(test[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(test[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "y_train = train['ons']\n",
    "y_xval = xval['ons']\n",
    "y_test = test['ons']\n",
    "\n",
    "np.save(\"final_nn/preprocessed_hr_X_train.npy\", X_train)\n",
    "np.save(\"final_nn/preprocessed_hr_X_xval.npy\", X_xval)\n",
    "np.save(\"final_nn/preprocessed_hr_X_test.npy\", X_test)\n",
    "np.save(\"final_nn/y_hr_train.npy\", y_train)\n",
    "np.save(\"final_nn/y_hr_xval.npy\", y_xval)\n",
    "np.save(\"final_nn/y_hr_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = list()\n",
    "for lab in X_NUM_COLS:\n",
    "    column_labels.append(f'num: {lab}')\n",
    "\n",
    "for i, cat in enumerate(X_CAT_COLS):\n",
    "    for cat_val in one_hot_encoder.categories_[i]:\n",
    "        column_labels.append(f'{cat}: {cat_val}')\n",
    "\n",
    "assert len(column_labels) == X_train.shape[1], f\"Len of column labels {len(column_labels)} matches dimension of training set {X_train.shape}\"\n",
    "\n",
    "import pickle\n",
    "with open('final_nn/hr_one_hot_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(one_hot_encoder, f)\n",
    "\n",
    "with open('final_nn/hr_standard_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('final_nn/hr_column_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(column_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting model with an initial layer of width 955, dropout=0.26130464669525444 and lr=0.9597436993934857\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 26s 104us/step - loss: 16.4086 - val_loss: 12.1715\nEpoch 2/25\n250334/250334 [==============================] - 28s 111us/step - loss: 13.1495 - val_loss: 12.7758\nRestoring model weights from the end of the best epoch\nEpoch 00002: early stopping\n31572/31572 [==============================] - 2s 62us/step\nFitting model with an initial layer of width 876, dropout=0.11736573673602974 and lr=0.828347108581263\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 25s 100us/step - loss: 14.4533 - val_loss: 11.6517\nEpoch 2/25\n250334/250334 [==============================] - 24s 95us/step - loss: 11.7358 - val_loss: 11.0432\nEpoch 3/25\n250334/250334 [==============================] - 23s 94us/step - loss: 11.1577 - val_loss: 10.7500\nEpoch 4/25\n250334/250334 [==============================] - 24s 94us/step - loss: 10.8032 - val_loss: 11.1258\nRestoring model weights from the end of the best epoch\nEpoch 00004: early stopping\n31572/31572 [==============================] - 2s 56us/step\nFitting model with an initial layer of width 1858, dropout=0.2577554468390668 and lr=0.654606167628466\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 44s 177us/step - loss: 14.7706 - val_loss: 11.6987\nEpoch 2/25\n250334/250334 [==============================] - 44s 175us/step - loss: 11.8506 - val_loss: 12.4162\nRestoring model weights from the end of the best epoch\nEpoch 00002: early stopping\n31572/31572 [==============================] - 3s 92us/step\nFitting model with an initial layer of width 1072, dropout=0.20718372745818198 and lr=0.22859313655228541\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 29s 116us/step - loss: 14.4747 - val_loss: 12.3929\nEpoch 2/25\n250334/250334 [==============================] - 28s 112us/step - loss: 11.5697 - val_loss: 11.1897\nEpoch 3/25\n250334/250334 [==============================] - 28s 113us/step - loss: 10.7808 - val_loss: 10.4489\nEpoch 4/25\n250334/250334 [==============================] - 28s 111us/step - loss: 10.3346 - val_loss: 10.3773\nEpoch 5/25\n250334/250334 [==============================] - 28s 112us/step - loss: 10.0498 - val_loss: 9.8529\nEpoch 6/25\n250334/250334 [==============================] - 28s 111us/step - loss: 9.8496 - val_loss: 9.7346\nEpoch 7/25\n250334/250334 [==============================] - 28s 112us/step - loss: 9.7105 - val_loss: 9.5873\nEpoch 8/25\n250334/250334 [==============================] - 28s 112us/step - loss: 9.6278 - val_loss: 9.4401\nEpoch 9/25\n250334/250334 [==============================] - 28s 112us/step - loss: 9.5271 - val_loss: 9.4788\nRestoring model weights from the end of the best epoch\nEpoch 00009: early stopping\n31572/31572 [==============================] - 2s 66us/step\nFitting model with an initial layer of width 1830, dropout=0.24621735658003485 and lr=0.6907359192669341\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 43s 173us/step - loss: 15.1731 - val_loss: 11.7074\nEpoch 2/25\n250334/250334 [==============================] - 42s 169us/step - loss: 12.0120 - val_loss: 10.8389\nEpoch 3/25\n250334/250334 [==============================] - 42s 168us/step - loss: 11.4670 - val_loss: 10.7628\nEpoch 4/25\n250334/250334 [==============================] - 42s 169us/step - loss: 11.1197 - val_loss: 10.3274\nEpoch 5/25\n250334/250334 [==============================] - 42s 169us/step - loss: 10.8509 - val_loss: 10.8254\nRestoring model weights from the end of the best epoch\nEpoch 00005: early stopping\n31572/31572 [==============================] - 3s 91us/step\nFitting model with an initial layer of width 1565, dropout=0.16571470181061093 and lr=0.22875119210662498\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 38s 153us/step - loss: 14.7073 - val_loss: 12.3289\nEpoch 2/25\n250334/250334 [==============================] - 37s 149us/step - loss: 11.6073 - val_loss: 11.2356\nEpoch 3/25\n250334/250334 [==============================] - 37s 149us/step - loss: 10.7876 - val_loss: 10.7472\nEpoch 4/25\n250334/250334 [==============================] - 37s 149us/step - loss: 10.2965 - val_loss: 10.0460\nEpoch 5/25\n250334/250334 [==============================] - 38s 150us/step - loss: 9.9765 - val_loss: 9.8393\nEpoch 6/25\n250334/250334 [==============================] - 38s 150us/step - loss: 9.7788 - val_loss: 9.6900\nEpoch 7/25\n250334/250334 [==============================] - 37s 149us/step - loss: 9.6315 - val_loss: 9.5905\nEpoch 8/25\n250334/250334 [==============================] - 38s 150us/step - loss: 9.5360 - val_loss: 9.5713\nEpoch 9/25\n250334/250334 [==============================] - 38s 150us/step - loss: 9.4593 - val_loss: 9.3468\nEpoch 10/25\n250334/250334 [==============================] - 38s 150us/step - loss: 9.3987 - val_loss: 9.3257\nEpoch 11/25\n250334/250334 [==============================] - 37s 149us/step - loss: 9.3260 - val_loss: 9.4615\nRestoring model weights from the end of the best epoch\nEpoch 00011: early stopping\n31572/31572 [==============================] - 3s 81us/step\nFitting model with an initial layer of width 2000, dropout=0.1 and lr=0.22902961281671236\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 46s 185us/step - loss: 14.8235 - val_loss: 12.2884\nEpoch 2/25\n250334/250334 [==============================] - 44s 177us/step - loss: 11.5727 - val_loss: 11.1173\nEpoch 3/25\n250334/250334 [==============================] - 43s 173us/step - loss: 10.6845 - val_loss: 10.6463\nEpoch 4/25\n250334/250334 [==============================] - 43s 171us/step - loss: 10.1574 - val_loss: 10.1597\nEpoch 5/25\n250334/250334 [==============================] - 43s 170us/step - loss: 9.8337 - val_loss: 9.8238\nEpoch 6/25\n250334/250334 [==============================] - 43s 170us/step - loss: 9.6352 - val_loss: 9.6277\nEpoch 7/25\n250334/250334 [==============================] - 43s 171us/step - loss: 9.4992 - val_loss: 9.4921\nEpoch 8/25\n250334/250334 [==============================] - 42s 170us/step - loss: 9.3867 - val_loss: 9.3708\nEpoch 9/25\n250334/250334 [==============================] - 43s 171us/step - loss: 9.2970 - val_loss: 9.4785\nRestoring model weights from the end of the best epoch\nEpoch 00009: early stopping\n31572/31572 [==============================] - 3s 91us/step\nFitting model with an initial layer of width 1939, dropout=0.10535953145580215 and lr=0.2334248151418024\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 43s 171us/step - loss: 14.7867 - val_loss: 12.6023\nEpoch 2/25\n250334/250334 [==============================] - 42s 169us/step - loss: 11.5238 - val_loss: 10.9457\nEpoch 3/25\n250334/250334 [==============================] - 42s 166us/step - loss: 10.6696 - val_loss: 10.4747\nEpoch 4/25\n250334/250334 [==============================] - 42s 167us/step - loss: 10.1491 - val_loss: 9.9316\nEpoch 5/25\n250334/250334 [==============================] - 42s 167us/step - loss: 9.8345 - val_loss: 9.8779\nEpoch 6/25\n250334/250334 [==============================] - 41s 165us/step - loss: 9.6248 - val_loss: 9.9804\nRestoring model weights from the end of the best epoch\nEpoch 00006: early stopping\n31572/31572 [==============================] - 3s 96us/step\nFitting model with an initial layer of width 2000, dropout=0.10049682394338234 and lr=0.22683468700657314\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 44s 176us/step - loss: 14.8757 - val_loss: 12.2744\nEpoch 2/25\n250334/250334 [==============================] - 43s 171us/step - loss: 11.5818 - val_loss: 10.9336\nEpoch 3/25\n250334/250334 [==============================] - 43s 171us/step - loss: 10.6938 - val_loss: 10.5446\nEpoch 4/25\n250334/250334 [==============================] - 44s 176us/step - loss: 10.2086 - val_loss: 10.0393\nEpoch 5/25\n250334/250334 [==============================] - 43s 172us/step - loss: 9.8457 - val_loss: 9.6600\nEpoch 6/25\n250334/250334 [==============================] - 43s 171us/step - loss: 9.6270 - val_loss: 9.6962\nRestoring model weights from the end of the best epoch\nEpoch 00006: early stopping\n31572/31572 [==============================] - 3s 92us/step\nFitting model with an initial layer of width 2000, dropout=0.5 and lr=0.22985244382810216\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 44s 177us/step - loss: 15.3337 - val_loss: 12.4864\nEpoch 2/25\n250334/250334 [==============================] - 43s 172us/step - loss: 12.3783 - val_loss: 11.4188\nEpoch 3/25\n250334/250334 [==============================] - 43s 172us/step - loss: 11.5820 - val_loss: 10.9821\nEpoch 4/25\n250334/250334 [==============================] - 44s 175us/step - loss: 11.2033 - val_loss: 10.5715\nEpoch 5/25\n250334/250334 [==============================] - 43s 172us/step - loss: 10.8823 - val_loss: 10.1151\nEpoch 6/25\n250334/250334 [==============================] - 43s 172us/step - loss: 10.6725 - val_loss: 10.4762\nRestoring model weights from the end of the best epoch\nEpoch 00006: early stopping\n31572/31572 [==============================] - 3s 92us/step\nFitting model with an initial layer of width 984, dropout=0.131087365888379 and lr=0.22966535570412147\nTrain on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 29s 116us/step - loss: 14.3174 - val_loss: 12.1265\nEpoch 2/25\n250334/250334 [==============================] - 28s 112us/step - loss: 11.4078 - val_loss: 10.9872\nEpoch 3/25\n 68352/250334 [=======>......................] - ETA: 20s - loss: 10.8050"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-15b80c1fe586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     ],\n\u001b[1;32m     22\u001b[0m     \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mn_random_starts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         callback=callback, n_jobs=n_jobs, model_queue_size=model_queue_size)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mnext_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-15b80c1fe586>\u001b[0m in \u001b[0;36mmodel_fit\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_xval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_xval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_xval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from skopt import gp_minimize\n",
    "\n",
    "def model_fit(params):\n",
    "    size, dropout, lr = params\n",
    "    max_iter = 25\n",
    "    print(f\"Fitting model with an initial layer of width {size}, dropout={dropout} and lr={lr}\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, activation=\"sigmoid\", input_dim=X_train.shape[1]))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=max_iter, batch_size=256, callbacks=[es])\n",
    "    return model.evaluate(X_xval, y_xval)\n",
    "\n",
    "best = gp_minimize(model_fit, [\n",
    "        (800, 2000),  # size\n",
    "        (0.1, 0.5),   # dropout\n",
    "        (0.1, 1)      # lr\n",
    "    ],\n",
    "    n_calls=25,\n",
    "    n_random_starts=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 250334 samples, validate on 31572 samples\nEpoch 1/25\n250334/250334 [==============================] - 53s 210us/step - loss: 14.8621 - val_loss: 12.2950\nEpoch 2/25\n250334/250334 [==============================] - 46s 184us/step - loss: 11.6099 - val_loss: 11.0648\nEpoch 3/25\n250334/250334 [==============================] - 50s 200us/step - loss: 10.7366 - val_loss: 10.4459\nEpoch 4/25\n250334/250334 [==============================] - 53s 211us/step - loss: 10.2068 - val_loss: 10.2447\nEpoch 5/25\n250334/250334 [==============================] - 46s 186us/step - loss: 9.8592 - val_loss: 9.7304\nEpoch 6/25\n250334/250334 [==============================] - 44s 174us/step - loss: 9.6412 - val_loss: 9.6784\nEpoch 7/25\n250334/250334 [==============================] - 42s 169us/step - loss: 9.4873 - val_loss: 9.6185\nEpoch 8/25\n250334/250334 [==============================] - 47s 187us/step - loss: 9.3841 - val_loss: 9.4021\nEpoch 9/25\n250334/250334 [==============================] - 47s 189us/step - loss: 9.3006 - val_loss: 9.4142\nRestoring model weights from the end of the best epoch\nEpoch 00009: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1a49630518>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2000, activation=\"sigmoid\", input_dim=X_train.shape[1]))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.22902961281, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=25, batch_size=256, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "31534/31534 [==============================] - 3s 101us/step\n"
    },
    {
     "data": {
      "text/plain": "9.268288187515822"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"final_nn/model_hr.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"final_nn/model_hr_weights_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set predictions out of the better performing model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# with open(\"final_nn/model_hr.json\", \"r\") as json_file:\n",
    "#     model = model_from_json(json_file.read())\n",
    "\n",
    "# model.load_weights(\"final_nn/model_hr_weights_train.h5\")\n",
    "\n",
    "with open(\"../predictions/final_nn_hr_test.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_test).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')\n",
    "\n",
    "with open(\"../predictions/final_nn_hr_xval.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_xval).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}