{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.2-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36264bitbaseconda7b74234baf454d23886bd31545e276ad",
   "display_name": "Python 3.6.2 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n3.6.2 |Anaconda custom (64-bit)| (default, Sep 21 2017, 18:29:43) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
    }
   ],
   "source": [
    "from sys import version\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from os import makedirs\n",
    "makedirs(\"final_nn\", exist_ok=True)\n",
    "\n",
    "print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training dimension: (542954, 20)\nXval dimension: (67876, 20)\nTest dimension: (67699, 20)\n"
    }
   ],
   "source": [
    "# Load and prepare training and xval data\n",
    "TRAINING_FILE, XVAL_FILE, TEST_FILE = \"../combined_data/15min/train.tsv.gz\", \"../combined_data/15min/xval.tsv.gz\", \"../combined_data/15min/test.tsv.gz\"\n",
    "train, xval, test = pd.read_csv(TRAINING_FILE, sep='\\t'), pd.read_csv(XVAL_FILE, sep='\\t'), pd.read_csv(TEST_FILE, sep='\\t')\n",
    "print(f'Training dimension: {train.shape}')\n",
    "print(f'Xval dimension: {xval.shape}')\n",
    "print(f'Test dimension: {test.shape}')\n",
    "#print('\\n'.join(train.columns))\n",
    "#train.head(n=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Creating X_train\nCreating X_xval\nCreating X_test\n"
    }
   ],
   "source": [
    "X_NUM_COLS = [\n",
    "    'orca_total', \n",
    "    'frac_disabled', \n",
    "    'frac_youth', \n",
    "    'frac_senior', \n",
    "    'frac_li', \n",
    "    'frac_uw'\n",
    "]\n",
    "X_CAT_COLS = [\n",
    "    'is_ns', \n",
    "    'is_rapid', \n",
    "    'is_weekend', \n",
    "    'trip_start_hr_15', \n",
    "    'rte', \n",
    "    'dir', \n",
    "    'day_of_week', \n",
    "    'region', \n",
    "    'start', \n",
    "    'end', \n",
    "    'summer'\n",
    "]\n",
    "\n",
    "#label_encoders = {col: LabelEncoder() for col in X_CAT_COLS}\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Creating X_train\")\n",
    "X_train = np.concatenate((\n",
    "    scaler.fit_transform(train[X_NUM_COLS]),\n",
    "    one_hot_encoder.fit_transform(train[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_xval\")\n",
    "X_xval = np.concatenate((\n",
    "    scaler.transform(xval[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(xval[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_test\")\n",
    "X_test = np.concatenate((\n",
    "    scaler.transform(test[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(test[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "y_train = train['ons']\n",
    "y_xval = xval['ons']\n",
    "y_test = test['ons']\n",
    "\n",
    "np.save(\"final_nn/preprocessed_15m_X_train.npy\", X_train)\n",
    "np.save(\"final_nn/preprocessed_15m_X_xval.npy\", X_xval)\n",
    "np.save(\"final_nn/preprocessed_15m_X_test.npy\", X_test)\n",
    "np.save(\"final_nn/y_15m_train.npy\", y_train)\n",
    "np.save(\"final_nn/y_15m_xval.npy\", y_xval)\n",
    "np.save(\"final_nn/y_15m_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder.categories_\n",
    "\n",
    "column_labels = list()\n",
    "for lab in X_NUM_COLS:\n",
    "    column_labels.append(f'num: {lab}')\n",
    "\n",
    "for i, cat in enumerate(X_CAT_COLS):\n",
    "    for cat_val in one_hot_encoder.categories_[i]:\n",
    "        column_labels.append(f'{cat}: {cat_val}')\n",
    "\n",
    "assert len(column_labels) == X_train.shape[1], f\"Len of column labels {len(column_labels)} matches dimension of training set {X_train.shape}\"\n",
    "\n",
    "import pickle\n",
    "with open('final_nn/15m_one_hot_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(one_hot_encoder, f)\n",
    "\n",
    "with open('final_nn/15m_standard_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('final_nn/15m_column_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(column_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 542954 samples, validate on 67876 samples\nEpoch 1/25\n542954/542954 [==============================] - 59s 108us/step - loss: 8.3371 - val_loss: 7.3632\nEpoch 2/25\n542954/542954 [==============================] - 56s 103us/step - loss: 7.3305 - val_loss: 7.0077\nEpoch 3/25\n542954/542954 [==============================] - 60s 110us/step - loss: 7.1107 - val_loss: 6.8927\nEpoch 4/25\n542954/542954 [==============================] - 53s 98us/step - loss: 6.9741 - val_loss: 6.7360\nEpoch 5/25\n542954/542954 [==============================] - 53s 98us/step - loss: 6.8731 - val_loss: 6.6872\nEpoch 6/25\n542954/542954 [==============================] - 49s 90us/step - loss: 6.8041 - val_loss: 6.6168\nEpoch 7/25\n542954/542954 [==============================] - 54s 99us/step - loss: 6.7521 - val_loss: 6.5967\nEpoch 8/25\n542954/542954 [==============================] - 52s 97us/step - loss: 6.7091 - val_loss: 6.5560\nEpoch 9/25\n542954/542954 [==============================] - 49s 90us/step - loss: 6.6774 - val_loss: 6.5172\nEpoch 10/25\n542954/542954 [==============================] - 52s 95us/step - loss: 6.6460 - val_loss: 6.5664\nRestoring model weights from the end of the best epoch\nEpoch 00010: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1a476bbd30>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(600, activation=\"sigmoid\", input_dim=426)) # 700: 6.59 / 6.62\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=25, batch_size=256, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "67699/67699 [==============================] - 2s 32us/step\nTest perf: 6.5460960049349515\n"
    }
   ],
   "source": [
    "print(f'Test perf: {model.evaluate(X_test, y_test, batch_size=128)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"final_nn/model_15min.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"final_nn/model_15min_weights_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(610830, 426)\nTrain on 610830 samples, validate on 67699 samples\nEpoch 1/25\n610830/610830 [==============================] - 66s 109us/step - loss: 8.2875 - val_loss: 7.6572\nEpoch 2/25\n610830/610830 [==============================] - 64s 105us/step - loss: 7.2869 - val_loss: 7.1783\nEpoch 3/25\n610830/610830 [==============================] - 68s 111us/step - loss: 7.0666 - val_loss: 7.1143\nEpoch 4/25\n610830/610830 [==============================] - 69s 113us/step - loss: 6.9263 - val_loss: 6.9405\nEpoch 5/25\n610830/610830 [==============================] - 73s 119us/step - loss: 6.8317 - val_loss: 6.8117\nEpoch 6/25\n610830/610830 [==============================] - 70s 114us/step - loss: 6.7723 - val_loss: 7.3357\nRestoring model weights from the end of the best epoch\nEpoch 00006: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1a48c9bc88>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a version on the training and xval sets combined, using the test set as validation.\n",
    "\n",
    "X_train = np.load(\"final_nn/preprocessed_X_train.npy\")\n",
    "X_xval = np.load(\"final_nn/preprocessed_X_xval.npy\")\n",
    "X_test = np.load(\"final_nn/preprocessed_X_test.npy\")\n",
    "y_train = np.load(\"final_nn/y_train.npy\")\n",
    "y_xval = np.load(\"final_nn/y_xval.npy\")\n",
    "y_test = np.load(\"final_nn/y_test.npy\")\n",
    "\n",
    "X_bigtrain = np.concatenate([X_train,X_xval])\n",
    "print(X_bigtrain.shape)\n",
    "y_bigtrain = np.concatenate([y_train,y_xval])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(800, activation=\"sigmoid\", input_dim=426))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "model.fit(X_bigtrain, y_bigtrain, validation_data=(X_test, y_test), epochs=25, batch_size=256, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"final_nn/model_15min_weights_train_and_xval.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set predictions out of the better performing model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open(\"final_nn/model_15min.json\", \"r\") as json_file:\n",
    "    model = model_from_json(json_file.read())\n",
    "\n",
    "model.load_weights(\"final_nn/model_15min_weights_train.h5\")\n",
    "\n",
    "with open(\"../predictions/final_nn_15min_test.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_test).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')\n",
    "\n",
    "with open(\"../predictions/final_nn_15min_xval.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_xval).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training dimension: (424373, 20)\nXval dimension: (53071, 20)\nTest dimension: (53169, 20)\nCreating X_train\nCreating X_xval\nCreating X_test\n"
    }
   ],
   "source": [
    "# Fit another model on 30m data.\n",
    "# Load and prepare training and xval data\n",
    "TRAINING_FILE, XVAL_FILE, TEST_FILE = \"../combined_data/30min/train.tsv.gz\", \"../combined_data/30min/xval.tsv.gz\", \"../combined_data/30min/test.tsv.gz\"\n",
    "train, xval, test = pd.read_csv(TRAINING_FILE, sep='\\t'), pd.read_csv(XVAL_FILE, sep='\\t'), pd.read_csv(TEST_FILE, sep='\\t')\n",
    "print(f'Training dimension: {train.shape}')\n",
    "print(f'Xval dimension: {xval.shape}')\n",
    "print(f'Test dimension: {test.shape}')\n",
    "X_NUM_COLS = [\n",
    "    'orca_total', \n",
    "    'frac_disabled', \n",
    "    'frac_youth', \n",
    "    'frac_senior', \n",
    "    'frac_li', \n",
    "    'frac_uw'\n",
    "]\n",
    "X_CAT_COLS = [\n",
    "    'is_ns', \n",
    "    'is_rapid', \n",
    "    'is_weekend', \n",
    "    'trip_start_hr_30',\n",
    "    'rte', \n",
    "    'dir', \n",
    "    'day_of_week', \n",
    "    'region', \n",
    "    'start', \n",
    "    'end', \n",
    "    'summer'\n",
    "]\n",
    "\n",
    "#label_encoders = {col: LabelEncoder() for col in X_CAT_COLS}\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Creating X_train\")\n",
    "X_train = np.concatenate((\n",
    "    scaler.fit_transform(train[X_NUM_COLS]),\n",
    "    one_hot_encoder.fit_transform(train[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_xval\")\n",
    "X_xval = np.concatenate((\n",
    "    scaler.transform(xval[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(xval[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_test\")\n",
    "X_test = np.concatenate((\n",
    "    scaler.transform(test[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(test[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "y_train = train['ons']\n",
    "y_xval = xval['ons']\n",
    "y_test = test['ons']\n",
    "\n",
    "np.save(\"final_nn/preprocessed_30m_X_train.npy\", X_train)\n",
    "np.save(\"final_nn/preprocessed_30m_X_xval.npy\", X_xval)\n",
    "np.save(\"final_nn/preprocessed_30m_X_test.npy\", X_test)\n",
    "np.save(\"final_nn/y_30m_train.npy\", y_train)\n",
    "np.save(\"final_nn/y_30m_xval.npy\", y_xval)\n",
    "np.save(\"final_nn/y_30m_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = list()\n",
    "for lab in X_NUM_COLS:\n",
    "    column_labels.append(f'num: {lab}')\n",
    "\n",
    "for i, cat in enumerate(X_CAT_COLS):\n",
    "    for cat_val in one_hot_encoder.categories_[i]:\n",
    "        column_labels.append(f'{cat}: {cat_val}')\n",
    "\n",
    "assert len(column_labels) == X_train.shape[1], f\"Len of column labels {len(column_labels)} matches dimension of training set {X_train.shape}\"\n",
    "\n",
    "import pickle\n",
    "with open('final_nn/30m_one_hot_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(one_hot_encoder, f)\n",
    "\n",
    "with open('final_nn/30m_standard_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('final_nn/30m_column_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(column_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 424373 samples, validate on 53071 samples\nEpoch 1/25\n424373/424373 [==============================] - 34s 79us/step - loss: 10.6632 - val_loss: 9.3755\nEpoch 2/25\n424373/424373 [==============================] - 33s 78us/step - loss: 9.1273 - val_loss: 8.8263\nEpoch 3/25\n424373/424373 [==============================] - 33s 78us/step - loss: 8.7464 - val_loss: 8.5684\nEpoch 4/25\n424373/424373 [==============================] - 34s 81us/step - loss: 8.5036 - val_loss: 8.2456\nEpoch 5/25\n424373/424373 [==============================] - 31s 74us/step - loss: 8.3329 - val_loss: 8.0746\nEpoch 6/25\n424373/424373 [==============================] - 32s 75us/step - loss: 8.2283 - val_loss: 8.0106\nEpoch 7/25\n424373/424373 [==============================] - 33s 77us/step - loss: 8.1558 - val_loss: 7.9559\nEpoch 8/25\n424373/424373 [==============================] - 31s 74us/step - loss: 8.0891 - val_loss: 7.9433\nEpoch 9/25\n424373/424373 [==============================] - 32s 75us/step - loss: 8.0358 - val_loss: 7.8799\nEpoch 10/25\n424373/424373 [==============================] - 32s 75us/step - loss: 7.9907 - val_loss: 7.8123\nEpoch 11/25\n424373/424373 [==============================] - 32s 75us/step - loss: 7.9613 - val_loss: 7.8556\nRestoring model weights from the end of the best epoch\nEpoch 00011: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1a464ab160>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, activation=\"sigmoid\", input_dim=378))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=25, batch_size=256, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"final_nn/model_30min.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"final_nn/model_30min_weights_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(477444, 378)\nTrain on 424373 samples, validate on 53169 samples\nEpoch 1/25\n424373/424373 [==============================] - 38s 90us/step - loss: 10.3069 - val_loss: 9.0088\nEpoch 2/25\n424373/424373 [==============================] - 38s 89us/step - loss: 8.9685 - val_loss: 8.6331\nEpoch 3/25\n424373/424373 [==============================] - 39s 91us/step - loss: 8.6260 - val_loss: 8.6365\nRestoring model weights from the end of the best epoch\nEpoch 00003: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1a4751c438>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bigtrain = np.concatenate([X_train,X_xval])\n",
    "print(X_bigtrain.shape)\n",
    "y_bigtrain = np.concatenate([y_train,y_xval])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(600, activation=\"sigmoid\", input_dim=378))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.4, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=256, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set predictions out of the better performing model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open(\"final_nn/model_30min.json\", \"r\") as json_file:\n",
    "    model = model_from_json(json_file.read())\n",
    "\n",
    "model.load_weights(\"final_nn/model_30min_weights_train.h5\")\n",
    "\n",
    "with open(\"../predictions/final_nn_30min_test.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_test).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')\n",
    "\n",
    "with open(\"../predictions/final_nn_30min_xval.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_xval).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training dimension: (272646, 20)\nXval dimension: (34382, 20)\nTest dimension: (34258, 20)\nCreating X_train\nCreating X_xval\nCreating X_test\n"
    }
   ],
   "source": [
    "# And for 1hr\n",
    "\n",
    "TRAINING_FILE, XVAL_FILE, TEST_FILE = \"../combined_data/hr/train.tsv.gz\", \"../combined_data/hr/xval.tsv.gz\", \"../combined_data/hr/test.tsv.gz\"\n",
    "train, xval, test = pd.read_csv(TRAINING_FILE, sep='\\t'), pd.read_csv(XVAL_FILE, sep='\\t'), pd.read_csv(TEST_FILE, sep='\\t')\n",
    "print(f'Training dimension: {train.shape}')\n",
    "print(f'Xval dimension: {xval.shape}')\n",
    "print(f'Test dimension: {test.shape}')\n",
    "X_NUM_COLS = [\n",
    "    'orca_total', \n",
    "    'frac_disabled', \n",
    "    'frac_youth', \n",
    "    'frac_senior', \n",
    "    'frac_li', \n",
    "    'frac_uw'\n",
    "]\n",
    "X_CAT_COLS = [\n",
    "    'is_ns', \n",
    "    'is_rapid', \n",
    "    'is_weekend', \n",
    "    'trip_start_hr',\n",
    "    'rte', \n",
    "    'dir', \n",
    "    'day_of_week', \n",
    "    'region', \n",
    "    'start', \n",
    "    'end', \n",
    "    'summer'\n",
    "]\n",
    "\n",
    "#label_encoders = {col: LabelEncoder() for col in X_CAT_COLS}\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Creating X_train\")\n",
    "X_train = np.concatenate((\n",
    "    scaler.fit_transform(train[X_NUM_COLS]),\n",
    "    one_hot_encoder.fit_transform(train[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_xval\")\n",
    "X_xval = np.concatenate((\n",
    "    scaler.transform(xval[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(xval[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "print(\"Creating X_test\")\n",
    "X_test = np.concatenate((\n",
    "    scaler.transform(test[X_NUM_COLS]),\n",
    "    one_hot_encoder.transform(test[X_CAT_COLS]).todense()\n",
    "), axis=1)\n",
    "\n",
    "y_train = train['ons']\n",
    "y_xval = xval['ons']\n",
    "y_test = test['ons']\n",
    "\n",
    "np.save(\"final_nn/preprocessed_hr_X_train.npy\", X_train)\n",
    "np.save(\"final_nn/preprocessed_hr_X_xval.npy\", X_xval)\n",
    "np.save(\"final_nn/preprocessed_hr_X_test.npy\", X_test)\n",
    "np.save(\"final_nn/y_hr_train.npy\", y_train)\n",
    "np.save(\"final_nn/y_hr_xval.npy\", y_xval)\n",
    "np.save(\"final_nn/y_hr_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = list()\n",
    "for lab in X_NUM_COLS:\n",
    "    column_labels.append(f'num: {lab}')\n",
    "\n",
    "for i, cat in enumerate(X_CAT_COLS):\n",
    "    for cat_val in one_hot_encoder.categories_[i]:\n",
    "        column_labels.append(f'{cat}: {cat_val}')\n",
    "\n",
    "assert len(column_labels) == X_train.shape[1], f\"Len of column labels {len(column_labels)} matches dimension of training set {X_train.shape}\"\n",
    "\n",
    "import pickle\n",
    "with open('final_nn/hr_one_hot_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(one_hot_encoder, f)\n",
    "\n",
    "with open('final_nn/hr_standard_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open('final_nn/hr_column_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(column_labels, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 272646 samples, validate on 34382 samples\nEpoch 1/25\n272646/272646 [==============================] - 19s 71us/step - loss: 16.7689 - val_loss: 14.8018\nEpoch 2/25\n272646/272646 [==============================] - 17s 62us/step - loss: 13.3861 - val_loss: 14.1845\nEpoch 3/25\n272646/272646 [==============================] - 17s 63us/step - loss: 12.5700 - val_loss: 13.7044\nEpoch 4/25\n272646/272646 [==============================] - 17s 63us/step - loss: 12.1537 - val_loss: 12.0574\nEpoch 5/25\n272646/272646 [==============================] - 18s 66us/step - loss: 11.8273 - val_loss: 14.3926\nRestoring model weights from the end of the best epoch\nEpoch 00005: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1a47191f98>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(450, activation=\"sigmoid\", input_dim=354))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.15, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=25, batch_size=256, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "34258/34258 [==============================] - 2s 66us/step\n"
    },
    {
     "data": {
      "text/plain": "11.88932574933197"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 272646 samples, validate on 34382 samples\nEpoch 1/25\n272646/272646 [==============================] - 22s 80us/step - loss: 16.8358 - val_loss: 15.1409\nEpoch 2/25\n272646/272646 [==============================] - 22s 81us/step - loss: 13.4072 - val_loss: 13.0852\nEpoch 3/25\n272646/272646 [==============================] - 24s 87us/step - loss: 12.5642 - val_loss: 12.5421\nEpoch 4/25\n272646/272646 [==============================] - 22s 80us/step - loss: 12.1385 - val_loss: 12.2758\nEpoch 5/25\n272646/272646 [==============================] - 22s 81us/step - loss: 11.7891 - val_loss: 11.9793\nEpoch 6/25\n272646/272646 [==============================] - 22s 81us/step - loss: 11.5006 - val_loss: 11.4586\nEpoch 7/25\n272646/272646 [==============================] - 21s 76us/step - loss: 11.2966 - val_loss: 11.3949\nEpoch 8/25\n272646/272646 [==============================] - 22s 79us/step - loss: 11.1469 - val_loss: 12.7752\nRestoring model weights from the end of the best epoch\nEpoch 00008: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x13db63c18>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, activation=\"sigmoid\", input_dim=354))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "sgd = SGD(lr=0.15, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_xval, y_xval), epochs=25, batch_size=256, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "34258/34258 [==============================] - 2s 68us/step\n"
    },
    {
     "data": {
      "text/plain": "11.195544252101104"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"final_nn/model_hr.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.save_weights(\"final_nn/model_hr_weights_train.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set predictions out of the better performing model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open(\"final_nn/model_hr.json\", \"r\") as json_file:\n",
    "    model = model_from_json(json_file.read())\n",
    "\n",
    "model.load_weights(\"final_nn/model_hr_weights_train.h5\")\n",
    "\n",
    "with open(\"../predictions/final_nn_hr_test.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_test).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')\n",
    "\n",
    "with open(\"../predictions/final_nn_hr_xval.txt\", 'wt') as f:\n",
    "    for val in model.predict(X_xval).squeeze():\n",
    "        f.write(f'{val:.15f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}